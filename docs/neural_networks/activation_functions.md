### Функции активации

1. **ReLU (Rectified Linear Unit)**
   - **Описание:** Функция активации ReLU используется для введения нелинейности в модель. Она заменяет все отрицательные значения нулями, оставляя положительные значения без изменений.
   - **Пример из кода:** В вашем проекте функция ReLU может быть использована в слое модели, например:
     ```python
     x = F.relu(self.bn1(self.conv1(x)))
     ```

2. **Softmax**
   - **Описание:** Функция активации Softmax преобразует выходные значения модели в вероятностное распределение. Это полезно для задач классификации, где нужно выбрать наиболее вероятный класс.
   - **Пример из кода:** В случае использования модели для оценки позиции или выбора хода, Softmax может применяться для получения вероятностей классов:
     ```python
     _, predicted = torch.max(outputs, 1)
     ```

### Примеры из кода

- **ReLU:** Пример из кода модели для активации выходных значений сверточного слоя:
  ```python
  x = F.relu(self.bn1(self.conv1(x)))
  ```
  Здесь `F.relu` применяется к результату после нормализации и свертки, чтобы ввести нелинейность.

- **Softmax:** Пример использования для выбора наиболее вероятного хода:
  ```python
  _, predicted = torch.max(outputs, 1)
  ```
  Здесь `torch.max` применяется к выходу модели, который уже преобразован с помощью Softmax (в рамках функции потерь Cross-Entropy Loss).

Эти функции активации играют ключевую роль в обучении и работе модели, обеспечивая нелинейность и возможность классификации.
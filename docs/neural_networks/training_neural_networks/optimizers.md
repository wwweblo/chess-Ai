### Оптимизаторы в проекте

1. **Adam (Adaptive Moment Estimation)**:
   - **Описание**: Один из наиболее популярных оптимизаторов для нейронных сетей, который сочетает в себе преимущества двух других методов – градиентного спуска с моментом и RMSProp. Adam адаптирует скорость обучения для каждого параметра, используя оценки первых и вторых моментов градиентов.
   - **Пример из кода**:
     ```python
     optimizer = optim.Adam(model.parameters(), lr=learning_rate)
     ```
     В этом примере `optim.Adam` используется для оптимизации параметров модели `ChessNet`, где `learning_rate` определяет скорость обучения.

2. **Scheduler (Learning Rate Scheduler)**:
   - **Описание**: Scheduler позволяет динамически изменять скорость обучения в процессе обучения, что может улучшить производительность модели. В проекте используется `StepLR`, который уменьшает скорость обучения на определенное значение каждые несколько эпох.
   - **Пример из кода**:
     ```python
     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
     ```
     В этом примере `StepLR` уменьшает скорость обучения на фактор `gamma` (0.5) каждые `step_size` эпох (5), что помогает адаптировать обучение модели по мере прогресса. 

Эти оптимизаторы и их настройки помогают эффективно обучать модель, улучшая её производительность и обеспечивая стабильное обучение.
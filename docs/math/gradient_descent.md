В проекте использованы ключевые понятия градиентного спуска, такие как:

1. **Функция потерь (loss function)** – это мера, по которой оценивается качество предсказаний нейросети. В коде используется функция `CrossEntropyLoss` для классификации:

    ```python
    criterion = nn.CrossEntropyLoss()
    ```

2. **Вычисление градиентов (backpropagation)** – процесс, с помощью которого нейросеть обновляет свои параметры на основе градиента функции потерь. Это реализуется через вызов метода `backward()`:

    ```python
    loss.backward()
    ```

3. **Обновление параметров (optimization step)** – градиенты, рассчитанные на предыдущем шаге, используются для обновления параметров модели с помощью оптимизатора (в данном случае, Adam):

    ```python
    optimizer.step()
    ```

4. **Шаг обучения (learning rate)** – это гиперпараметр, который определяет величину обновления параметров. Используется в настройке оптимизатора:

    ```python
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    ```

Эти шаги позволяют модели уменьшать функцию потерь, корректируя свои веса, чтобы улучшать предсказания с каждой итерацией обучения.